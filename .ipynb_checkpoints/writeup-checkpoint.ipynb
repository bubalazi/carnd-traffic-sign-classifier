{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CarND Project 2: Traffic Sign Recognition**\n",
    "Author: Lyuboslav Petrov\n",
    "\n",
    "<!--Image References-->\n",
    "[softmax]: ./doc/softmax.png \"Softmax probabilities\"\n",
    "[real-world]: ./doc/real-world.png \"Real World Accuracy\"\n",
    "[results]: ./doc/results.png \"Training Results after 50 Epochs\"\n",
    "[balanced_train]: ./doc/balanced_train.png \"Balanced out classes\"\n",
    "[balanced]: ./doc/balanced.png \"Balanced out classes\"\n",
    "[perturbations]: ./doc/perturbations.png \"Image Perturbations\"\n",
    "[classes]: ./doc/all_classes.png \"Examples of all classes\"\n",
    "[distribution]: ./doc/distribution.png \"Sample Size Distribution\"\n",
    "\n",
    "---\n",
    "## Summary\n",
    "\n",
    "This below outlines the work performed for analyzing the [German Traffic Sign Benchmark Dataset](http://benchmark.ini.rub.de/)  from the Ruhr-University, Bochum, Germany, as\n",
    "part of the [Self Driving Car Engineer](https://www.udacity.com/) nanodegree from Udacity.\n",
    "A convolutional neural network was trained with a validation accuracy of ~94% and testing\n",
    "accuracy of ~95%. Real-world testing with images from the internet showed results\n",
    "approaching 30% accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "One of the main characteristics of a legalized road are its signs and markings.\n",
    "It is therefore of great interest to the self-driving car research domain to\n",
    "find accurate, fast and resilient algorithms for image based road sign detection\n",
    "and classification. This work details the ***classification*** aspect, assuming the\n",
    "signs were already detected.\n",
    "\n",
    "---\n",
    "\n",
    "## Methods\n",
    "\n",
    "### Data Summary\n",
    "\n",
    "The dataset consists of labeled images organized in train, test and validation sets:\n",
    "  * Number of training examples: **34799**\n",
    "  * Number of testing examples: **4410**\n",
    "  * Image data shape: **32, 32, 3**\n",
    "  * Number of classes: **43**\n",
    "\n",
    "![][classes]\n",
    "\n",
    "It is evident from the above figure, that there is a great variation of brightness,\n",
    "contrast and resolution in the data. However, the targets/sings are brought to the\n",
    "image foreground and populate the centre of every sample with the majority of pixels,\n",
    "in most cases, belonging to the signs.\n",
    "\n",
    "The samples per class distribution of all the sets can be seen below:\n",
    "\n",
    "![][distribution]\n",
    "\n",
    "As can be seen, the distributions along the different sets are very close, but\n",
    "the distribution of samples among classes is of great variance. It was therefore\n",
    "necessary to balance-out the classes by generating *surrogate* data, based on the\n",
    "existing dataset.\n",
    "\n",
    "### Pre-processing Steps\n",
    "\n",
    "Although the images have already undergone preprocessing steps (ROI cropping),\n",
    "further preprocessing was seen an efficient method for optimizing performance.\n",
    "\n",
    "#### Class Balancing with Surrogate data\n",
    "\n",
    "Since sample distribution among classes was seen to be greatly varying, it was\n",
    "decided to augment the lower-sample-count classes with artificially created data.\n",
    "\n",
    "Another benefitial aspect in adding perturbations to the data is that in this manner\n",
    "the network becomes more robust and less likely to overfit.\n",
    "\n",
    "The methods for creation of this data where all based on perturbing the existing\n",
    "samples, where the perturbations chosen where:\n",
    "\n",
    "1. Image **Rotation** by +- 6 to 9 degrees around the image centre\n",
    "2. Image **Translation** by +- 3 pixels along the x and y axes\n",
    "3. Image **Affine** transformation\n",
    "4. Image **Perspective** warping\n",
    "\n",
    "![][perturbations]\n",
    "\n",
    "Using these techniques for image generation and class balancing, several balancing\n",
    "thresholds where tested, namely **median**, **mean** and **max** counts of all samples\n",
    "among the classes and the **max** threshold was chosen as final.\n",
    "\n",
    "The results from class balancing can be seen below.\n",
    "\n",
    "***NOTE:*** Images where converted to float32 and therefore their colorspace\n",
    "is depicted differently by matplotlib.\n",
    "\n",
    "![][balanced]\n",
    "\n",
    "The resulting distribution for the training is shown below, where the total training\n",
    "set size changed from 34799 to 86429, hence the surrogate data represents **~60%** of\n",
    "all training data!\n",
    "\n",
    "![][balanced_train]\n",
    "\n",
    "#### Grayscale and Normalization\n",
    "\n",
    "Images where then converted to grayscale and normalized between 0 and 1.\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "In order to iterate through multiple network architectures, it is necessary\n",
    "to make the network models parametric, so interdependencies between variables can\n",
    "be solved dynamically.\n",
    "\n",
    "Fist, the layer dimensions are sequentially defined. Example:\n",
    "\n",
    "    layers = {}\n",
    "    layers.update({\n",
    "        'c1':{\n",
    "            'd': n_channels * 9,\n",
    "            'fx': 5,\n",
    "            'fy': 5\n",
    "        }\n",
    "    })\n",
    "    layers.update({\n",
    "        'c2':{\n",
    "            'd': layers['c1']['d'] * 6,\n",
    "            'fx': 5,\n",
    "            'fy': 5\n",
    "        }   \n",
    "    })\n",
    "    layers.update({\n",
    "        'c3':{\n",
    "            'd': layers['c2']['d'] * 4,\n",
    "            'fx': 5,\n",
    "            'fy': 5\n",
    "        }   \n",
    "    })\n",
    "    layers.update({\n",
    "        'f0': {\n",
    "            # Resulting flat size = n_channels * 9 * 6 * 4 = 1 * 9 * 6 * 4 = 216\n",
    "            'in': layers['c3']['d'],\n",
    "            'out': 480\n",
    "        }  \n",
    "    })\n",
    "    layers.update({\n",
    "        'f1': {\n",
    "            'in': layers['f0']['out'],\n",
    "            'out': 240\n",
    "        }\n",
    "    })\n",
    "    layers.update({\n",
    "        'f2': {\n",
    "            'in': layers['f1']['out'],\n",
    "            'out': 43\n",
    "        }\n",
    "    })\n",
    "\n",
    "Next, the weight and bias objects (python dictionaries) are constructed:\n",
    "\n",
    "    weights = {\n",
    "        'wc1': tfhe((layers['c1']['fx'], layers['c1']['fy'], n_channels, layers['c1']['d'])),\n",
    "        'wc2': tfhe((layers['c2']['fx'], layers['c2']['fy'], layers['c1']['d'], layers['c2']['d'])),\n",
    "        'wc3': tfhe((layers['c3']['fx'], layers['c3']['fy'], layers['c2']['d'], layers['c3']['d'])),\n",
    "        'wf0': tfhe((layers['f0']['in'], layers['f0']['out'])),\n",
    "        'wf1': tfhe((layers['f1']['in'], layers['f1']['out'])),\n",
    "        'wf2': tfhe((layers['f2']['in'], layers['f2']['out']))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'bc1': tf.Variable(tf.zeros(layers['c1']['d'])),\n",
    "        'bc2': tf.Variable(tf.zeros(layers['c2']['d'])),\n",
    "        'bc3': tf.Variable(tf.zeros(layers['c3']['d'])),\n",
    "        'bf0': tf.Variable(tf.zeros(layers['f0']['out'])),\n",
    "        'bf1': tf.Variable(tf.zeros(layers['f1']['out'])),\n",
    "        'bf2': tf.Variable(tf.zeros(layers['f2']['out']))\n",
    "    }\n",
    "\n",
    "where, the *tfhe* function points to the initialization routine detialed in [1].\n",
    "\n",
    "The initial architecture chosen was LeNet's convolutional network as detailed [here](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81). The parameter tweaking and\n",
    "performance testing showed that stacking another convolutional layer is of greater\n",
    "benefit then increasing the number of parameters (i.e. depth vs width).\n",
    "\n",
    "The final architecture chosen is three subsequent convolutional layers with average pooling,\n",
    "equal strides and equal filter widths (**w_c(0,1,2) = 5x5**), and respective filter\n",
    "depths (**d_c(0,1,2) = 9, 54, 220**). The following layers chosen are three subsequent fully connected layers with widths respectively (**w_fc(0,1,2) = 480, 240, 43**).\n",
    "\n",
    "| # | Layer                 |     Description\t                              |  Output\n",
    "|:-:|:---------------------:|:---------------------------------------------:|:-------------------:|\n",
    "|1  | Input                 | Grayscale image                               | 32x32x1\n",
    "|2  | Convolution (5x5x9)   | 1x1 Stride, Valid Padding                     | 28x28x9\n",
    "|3  | ReLu                  |                                               | 28x28x9\n",
    "|4  | Average pooling       | 2x2 stride                                    | 14x14x9\n",
    "|5  | Convolution (5x5x54)  | 1x1 Stride, Valid Padding                     | 10x10x54\n",
    "|6  | ReLu                  |                                               | 10x10x54\n",
    "|7  | Average pooling       | 2x2 stride                                    | 5x5x54\n",
    "|8  | Convolution (5x5x216) | 1x1 Stride, Valid Padding                     | 1x1x216\n",
    "|9  | ReLu                  |                                               | 1x1x216\n",
    "|10 | Average pooling       | 2x2 stride                                    | 1x1x216\n",
    "|11 | Fully connected\t\t    | Flattened network (1x216)                     | 1x480\n",
    "|12 | Dropout       \t\t    | val=0.85                                      | 1x480\n",
    "|13 | Fully connected\t\t    |                                               | 1x240\n",
    "|14 | Dropout       \t\t    | val=0.85                                      | 1x240\n",
    "|15 | Fully connected\t\t    |                                               | 1x43\n",
    "\n",
    "### Train - Validate - Test\n",
    "\n",
    "The network was trained and optimized for **50 Epochs** with a **Batch Size of 128** using:\n",
    "\n",
    "| # | Layer                 |     Description\t                             |  Output\n",
    "|:-:|:---------------------:|:---------------------------------------------:|:-------------------:|\n",
    "| 1 | Softmax               | Cross Entropy with Logits                     | 1x43\n",
    "| 2 | Loss Operation        | Reduce entropy with mean                      | 1x43\n",
    "| 3 | Optimizer             | Adam Optimizer (learning_rate = 0.0007)       | 1x43\n",
    "\n",
    "---\n",
    "\n",
    "## Results\n",
    "\n",
    "### Validation Accuracy\n",
    "![][results]\n",
    "\n",
    "### Testing Accuracy\n",
    "The training accuracy achieved was in the range of 0.950-0.960\n",
    "\n",
    "### Real World Testing Accuracy\n",
    "\n",
    "Testing with images downloaded from a google image search with key-words: \"German traffic signs\"\n",
    "resulted in accuracy of **0.30**.\n",
    "\n",
    "![][real-world]\n",
    "\n",
    "The softmax probabilities 5 randomly chosen real-world images are as follows:\n",
    "\n",
    "![][softmax]\n",
    "\n",
    "## Discussion\n",
    "\n",
    "The task at hand was successfully completed by achieving a higher than the desired accuracy (testing and validation)\n",
    "of 93%. Further testing with random images from the internet showed the weakness\n",
    "of the model, namly that it expects the sign to be centrally located within the\n",
    "image, to be of a certain resolution and to occupy the bulk are within the image.\n",
    "Prior data augmenting, real-world results were with accuracy < 0.1. After augmentation, the results improved significantly.\n",
    "\n",
    "During training and testing it was noted that increasing the dropout rate above\n",
    "0.5 up to 0.9 had only benefitial effects. This can be attributed to the intentionally\n",
    "wide fully connected layers at the end, where a large dropout will still result in\n",
    "enough nodes left to achieve a good result.\n",
    "\n",
    "The conversion to grayscale proved itself also very valuable. This operation is essentially\n",
    "a pre-convolution that reduces the dimensionality of the input set, which in turn reduces\n",
    "drastically the requirements for the network size with negligable information loss.\n",
    "\n",
    "Increasing network depth was the final step following which the desired accuracy was exceeded.\n",
    "\n",
    "\n",
    "## Visualizing the network state\n",
    "N/A\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\", CoRR, 2015\n",
    "\n",
    "<!--  -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
