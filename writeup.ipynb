{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# **CarND Project 2: Traffic Sign Recognition**\n",
    "Author: Lyuboslav Petrov\n",
    "\n",
    "<!--Image References-->\n",
    "[snow]: ./data/de_traffic_signs/1_snow.png \"General Caution in Snow\"\n",
    "[noentry]: ./data/de_traffic_signs/2_noentry.png \"No Entry\"\n",
    "[noentry2]: ./data/de_traffic_signs/3_noentry.png \"No Entry drawing\"\n",
    "[roundabout]: ./data/de_traffic_signs/4_roundabout.png \"Roundabout\"\n",
    "[limit30]: ./data/de_traffic_signs/5_limit30.png \"Speed Limit 30\"\n",
    "[softmax]: ./doc/softmax.png \"Softmax probabilities\"\n",
    "[real-world]: ./doc/real-world.png \"Real World Accuracy\"\n",
    "[results]: ./doc/results.png \"Training Results after 50 Epochs\"\n",
    "[balanced_train]: ./doc/balanced_train.png \"Balanced out classes\"\n",
    "[balanced]: ./doc/balanced.png \"Balanced out classes\"\n",
    "[perturbations]: ./doc/perturbations.png \"Image Perturbations\"\n",
    "[classes]: ./doc/all_classes.png \"Examples of all classes\"\n",
    "[distribution]: ./doc/distribution.png \"Sample Size Distribution\"\n",
    "\n",
    "---\n",
    "## Summary\n",
    "\n",
    "This below outlines the work performed for analyzing the [German Traffic Sign Benchmark Dataset](http://benchmark.ini.rub.de/)  from the Ruhr-University, Bochum, Germany, as\n",
    "part of the [Self Driving Car Engineer](https://www.udacity.com/) nanodegree from Udacity.\n",
    "A convolutional neural network was trained with a validation accuracy of ~94% and testing\n",
    "accuracy of ~95%. Real-world testing with images from the internet showed results\n",
    "approaching 30% accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "One of the main characteristics of a legalized road are its signs and markings.\n",
    "It is therefore of great interest to the self-driving car research domain to\n",
    "find accurate, fast and resilient algorithms for image based road sign detection\n",
    "and classification. This work details the ***classification*** aspect, assuming the\n",
    "signs were already detected.\n",
    "\n",
    "---\n",
    "\n",
    "## Methods\n",
    "\n",
    "### Data Summary\n",
    "\n",
    "The dataset consists of labeled images organized in train, test and validation sets:\n",
    "  * Number of training examples: **34799**\n",
    "  * Number of testing examples: **4410**\n",
    "  * Image data shape: **32, 32, 3**\n",
    "  * Number of classes: **43**\n",
    "\n",
    "![][classes]\n",
    "\n",
    "It is evident from the above figure, that there is a great variation of brightness,\n",
    "contrast and resolution in the data. However, the targets/sings are brought to the\n",
    "image foreground and populate the centre of every sample with the majority of pixels,\n",
    "in most cases, belonging to the signs.\n",
    "\n",
    "The samples per class distribution of all the sets can be seen below:\n",
    "\n",
    "![][distribution]\n",
    "\n",
    "As can be seen, the distributions along the different sets are very close, but\n",
    "the distribution of samples among classes is of great variance. It was therefore\n",
    "necessary to balance-out the classes by generating *surrogate* data, based on the\n",
    "existing dataset.\n",
    "\n",
    "### Pre-processing Steps\n",
    "\n",
    "Although the images have already undergone preprocessing steps (ROI cropping),\n",
    "further preprocessing was seen an efficient method for optimizing performance.\n",
    "\n",
    "#### Class Balancing with Surrogate data\n",
    "\n",
    "Since sample distribution among classes was seen to be greatly varying, it was\n",
    "decided to augment the lower-sample-count classes with artificially created data.\n",
    "\n",
    "Another benefitial aspect in adding perturbations to the data is that in this manner\n",
    "the network becomes more robust and less likely to overfit.\n",
    "\n",
    "The methods for creation of this data where all based on perturbing the existing\n",
    "samples, where the perturbations chosen where:\n",
    "\n",
    "1. Image **Rotation** by +- 6 to 9 degrees around the image centre\n",
    "2. Image **Translation** by +- 3 pixels along the x and y axes\n",
    "3. Image **Affine** transformation\n",
    "4. Image **Perspective** warping\n",
    "\n",
    "![][perturbations]\n",
    "\n",
    "Using these techniques for image generation and class balancing, several balancing\n",
    "thresholds where tested, namely **median**, **mean** and **max** counts of all samples\n",
    "among the classes and the **max** threshold was chosen as final.\n",
    "\n",
    "The results from class balancing can be seen below.\n",
    "\n",
    "***NOTE:*** Images where converted to float32 and therefore their colorspace\n",
    "is depicted differently by matplotlib.\n",
    "\n",
    "![][balanced]\n",
    "\n",
    "The resulting distribution for the training is shown below, where the total training\n",
    "set size changed from 34799 to 86429, hence the surrogate data represents **~60%** of\n",
    "all training data!\n",
    "\n",
    "![][balanced_train]\n",
    "\n",
    "#### Grayscale and Normalization\n",
    "\n",
    "Images where then converted to grayscale and normalized between 0 and 1.\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "Sevaral network architectures were iterated through. First, the LeNet convolutional network was taken and adapted to work with the traffic sign data set - adapting it to 43 categories, instead of 10. On the first iterations it was\n",
    "observed that the 3 channels of the image do not contribute towards better accuracy and the pre-processing now included not only normalization, but also a colorspace conversion to grayscale. In addition, multiple filter sizes where tested with the LeNet architecture, when the necessity of paramtrization was recognized (see below). Further, two dropout layers were added after the first two Fully-Connected layers which brought the accuracy towards 0.8-0.9. Multiple filter depths were tested, and with filter depths of (64, 128) for the first two convolutional layers, the network reached 0.91 accuracy. A further test was made with addition of a third convolutional layer, where final results came to ~0.95 accuracy. \n",
    "\n",
    "Details of the layers dimensions can be found below.\n",
    "\n",
    "In order to iterate through multiple network architectures, it is necessary\n",
    "to make the network models parametric, so interdependencies between variables can\n",
    "be solved dynamically.\n",
    "\n",
    "Fist, the layer dimensions are sequentially defined. Example:\n",
    "\n",
    "    layers = {}\n",
    "    layers.update({\n",
    "        'c1':{\n",
    "            'd': n_channels * 9,\n",
    "            'fx': 5,\n",
    "            'fy': 5\n",
    "        }\n",
    "    })\n",
    "    layers.update({\n",
    "        'c2':{\n",
    "            'd': layers['c1']['d'] * 6,\n",
    "            'fx': 5,\n",
    "            'fy': 5\n",
    "        }   \n",
    "    })\n",
    "    layers.update({\n",
    "        'c3':{\n",
    "            'd': layers['c2']['d'] * 4,\n",
    "            'fx': 5,\n",
    "            'fy': 5\n",
    "        }   \n",
    "    })\n",
    "    layers.update({\n",
    "        'f0': {\n",
    "            # Resulting flat size = n_channels * 9 * 6 * 4 = 1 * 9 * 6 * 4 = 216\n",
    "            'in': layers['c3']['d'],\n",
    "            'out': 480\n",
    "        }  \n",
    "    })\n",
    "    layers.update({\n",
    "        'f1': {\n",
    "            'in': layers['f0']['out'],\n",
    "            'out': 240\n",
    "        }\n",
    "    })\n",
    "    layers.update({\n",
    "        'f2': {\n",
    "            'in': layers['f1']['out'],\n",
    "            'out': 43\n",
    "        }\n",
    "    })\n",
    "\n",
    "Next, the weight and bias objects (python dictionaries) are constructed:\n",
    "\n",
    "    weights = {\n",
    "        'wc1': tfhe((layers['c1']['fx'], layers['c1']['fy'], n_channels, layers['c1']['d'])),\n",
    "        'wc2': tfhe((layers['c2']['fx'], layers['c2']['fy'], layers['c1']['d'], layers['c2']['d'])),\n",
    "        'wc3': tfhe((layers['c3']['fx'], layers['c3']['fy'], layers['c2']['d'], layers['c3']['d'])),\n",
    "        'wf0': tfhe((layers['f0']['in'], layers['f0']['out'])),\n",
    "        'wf1': tfhe((layers['f1']['in'], layers['f1']['out'])),\n",
    "        'wf2': tfhe((layers['f2']['in'], layers['f2']['out']))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'bc1': tf.Variable(tf.zeros(layers['c1']['d'])),\n",
    "        'bc2': tf.Variable(tf.zeros(layers['c2']['d'])),\n",
    "        'bc3': tf.Variable(tf.zeros(layers['c3']['d'])),\n",
    "        'bf0': tf.Variable(tf.zeros(layers['f0']['out'])),\n",
    "        'bf1': tf.Variable(tf.zeros(layers['f1']['out'])),\n",
    "        'bf2': tf.Variable(tf.zeros(layers['f2']['out']))\n",
    "    }\n",
    "\n",
    "where, the *tfhe* function points to the initialization routine detialed in [1].\n",
    "\n",
    "The initial architecture chosen was LeNet's convolutional network as detailed [here](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81). The parameter tweaking and\n",
    "performance testing showed that stacking another convolutional layer is of greater\n",
    "benefit then increasing the number of parameters (i.e. depth vs width).\n",
    "\n",
    "The final architecture chosen is three subsequent convolutional layers with average pooling,\n",
    "equal strides and equal filter widths (**w_c(0,1,2) = 5x5**), and respective filter\n",
    "depths (**d_c(0,1,2) = 9, 54, 220**). The following layers chosen are three subsequent fully connected layers with widths respectively (**w_fc(0,1,2) = 480, 240, 43**).\n",
    "\n",
    "| # | Layer                 |     Description\t                              |  Output\n",
    "|:-:|:---------------------:|:---------------------------------------------:|:-------------------:|\n",
    "|1  | Input                 | Grayscale image                               | 32x32x1\n",
    "|2  | Convolution (5x5x9)   | 1x1 Stride, Valid Padding                     | 28x28x9\n",
    "|3  | ReLu                  |                                               | 28x28x9\n",
    "|4  | Average pooling       | 2x2 stride                                    | 14x14x9\n",
    "|5  | Convolution (5x5x54)  | 1x1 Stride, Valid Padding                     | 10x10x54\n",
    "|6  | ReLu                  |                                               | 10x10x54\n",
    "|7  | Average pooling       | 2x2 stride                                    | 5x5x54\n",
    "|8  | Convolution (5x5x216) | 1x1 Stride, Valid Padding                     | 1x1x216\n",
    "|9  | ReLu                  |                                               | 1x1x216\n",
    "|10 | Average pooling       | 2x2 stride                                    | 1x1x216\n",
    "|11 | Fully connected\t\t| Flattened network (1x216)                     | 1x480\n",
    "|12 | Dropout       \t\t| val=0.85                                      | 1x480\n",
    "|13 | Fully connected\t\t|                                               | 1x240\n",
    "|14 | Dropout       \t\t| val=0.85                                      | 1x240\n",
    "|15 | Fully connected\t\t|                                               | 1x43\n",
    "\n",
    "### Train - Validate - Test\n",
    "\n",
    "The network was trained and optimized for **50 Epochs** with a **Batch Size of 128** using:\n",
    "For each image, discuss what quality or qualities might be difficult to classify.\n",
    "| # | Layer                 |     Description\t                            |  Output\n",
    "|:-:|:---------------------:|:---------------------------------------------:|:-------------------:|\n",
    "| 1 | Softmax               | Cross Entropy with Logits                     | 1x43\n",
    "| 2 | Loss Operation        | Reduce entropy with mean                      | 1x43\n",
    "| 3 | Optimizer             | Adam Optimizer (learning_rate = 0.0007)       | 1x43\n",
    "\n",
    "---\n",
    "\n",
    "## Results\n",
    "\n",
    "### Validation Accuracy\n",
    "![][results]\n",
    "\n",
    "### Testing Accuracy\n",
    "The training accuracy achieved was in the range of 0.950-0.960\n",
    "\n",
    "### Real World Testing Accuracy\n",
    "\n",
    "Testing with images downloaded from a google image search with key-words: \"German traffic signs\"\n",
    "resulted in accuracy of **0.30**.\n",
    "\n",
    "![][real-world]\n",
    "\n",
    "The softmax probabilities 5 randomly chosen real-world images are as follows:\n",
    "\n",
    "![][softmax]\n",
    "\n",
    "The individual images can below be seen in full size with their supporting discussion.\n",
    "\n",
    "#### 1. General Caution in Snow\n",
    "\n",
    "![][snow]\n",
    "\n",
    "The top 5 probabilities are far away from correct.\n",
    "\n",
    "Difficulties for classification:\n",
    "1. Snow! This is an image for a General Caution sign in the winter, partially covered in snow.\n",
    "2. Size ratio - the sign area is much smaller than the complete image area (<< 0.5), whereas the training set had a sign to image size ratio of approx 0.5\n",
    "3. Multiple Signs and overlayed text\n",
    "\n",
    "#### 2. No Entry under a high angle\n",
    "\n",
    "![][noentry]\n",
    "\n",
    "Difficulties for classification:\n",
    "\n",
    "1. Sign centre is shifted towards the upper edge of the image\n",
    "2. The pose of the sign relative to the camera is not favorable to the algorithm\n",
    "3. Size ratio\n",
    "\n",
    "#### 3. No Entry drawing\n",
    "\n",
    "![][noentry2]\n",
    "\n",
    "This image is a drawing and is as expected classified with probability of 1.0 \n",
    "\n",
    "#### 4. Roundabout\n",
    "\n",
    "![][roundabout]\n",
    "\n",
    "The roundabout mandatory sign is as well classified with a high probability.\n",
    "\n",
    "#### 5. Small Limit 30\n",
    "\n",
    "![][limit30]\n",
    "\n",
    "Problems with this image are:\n",
    "\n",
    "1. Size ratio (sign area to image area)\n",
    "\n",
    "## Discussion\n",
    "\n",
    "The task at hand was successfully completed by achieving a higher than the desired accuracy (testing and validation)\n",
    "of 93%. Further testing with random images from the internet showed the weakness\n",
    "of the model, namly that it expects the sign to be centrally located within the\n",
    "image, to be of a certain resolution and to occupy the bulk are within the image.\n",
    "Prior data augmenting, real-world results were with accuracy < 0.1. After augmentation, the results improved significantly.\n",
    "\n",
    "During training and testing it was noted that increasing the dropout rate above\n",
    "0.5 up to 0.9 had only benefitial effects. This can be attributed to the intentionally\n",
    "wide fully connected layers at the end, where a large dropout will still result in\n",
    "enough nodes left to achieve a good result.\n",
    "\n",
    "The conversion to grayscale proved itself also very valuable. This operation is essentially\n",
    "a pre-convolution that reduces the dimensionality of the input set, which in turn reduces\n",
    "drastically the requirements for the network size with negligable information loss.\n",
    "\n",
    "Increasing network depth was the final step following which the desired accuracy was exceeded.\n",
    "\n",
    "\n",
    "## Visualizing the network state\n",
    "N/A\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\", CoRR, 2015\n",
    "\n",
    "<!--  -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
